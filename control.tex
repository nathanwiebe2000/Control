\documentclass[superscriptaddress,aps,pra,nofootinbib,onecolumn,notitlepage,10pt]{revtex4-1}
\pdfoutput=1
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{placeins}
\usepackage[caption=false]{subfig}
\usepackage[colorlinks]{hyperref}
\usepackage{hypcap}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\eq}[1]{Eq.~\hyperref[eq:#1]{(\ref*{eq:#1})}}
\renewcommand{\sec}[1]{\hyperref[sec:#1]{Section~\ref*{sec:#1}}}
\newcommand{\app}[1]{\hyperref[app:#1]{Appendix~\ref*{app:#1}}}
\newcommand{\tab}[1]{\hyperref[tab:#1]{Table~\ref*{tab:#1}}}
\newcommand{\fig}[1]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}}}
\newcommand{\figa}[2]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}#2}}
\newcommand{\figx}[2]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}(#2)}}
\newcommand{\thm}[1]{\hyperref[thm:#1]{Theorem~\ref*{thm:#1}}}
\newcommand{\lem}[1]{\hyperref[lem:#1]{Lemma~\ref*{lem:#1}}}
\newcommand{\cor}[1]{\hyperref[cor:#1]{Corollary~\ref*{cor:#1}}}
\newcommand{\defn}[1]{\hyperref[def:#1]{Definition~\ref*{def:#1}}}
\newcommand{\alg}[1]{\hyperref[alg:#1]{Algorithm~\ref*{alg:#1}}}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\newcommand{\tr}{\operatorname{Tr}}
\def\avg#1{\mathinner{\langle{#1}\rangle}}
\def\bra#1{\mathinner{\langle{#1}|}}
\def\ket#1{\mathinner{|{#1}\rangle}}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\ketbra}[2]{|#1\rangle\!\langle#2|}
\newcommand{\proj}[1]{\ket{#1}\!\!\bra{#1}}

\newcommand\R{{\mathrm {I\!R}}}
\newcommand\N{{\mathrm {I\!N}}}
\newcommand\h{{\cal H}}
\newcommand\V{{\cal V}}
\newcommand\p{{\sf p}}
\newcommand\w{{\sf w}}

%\newcommand{\qed}{$\hfill \Box$}
\newcommand\diag{{\mbox{diag\,}}}
\def\half{\tfrac{1}{2}}
\newcommand{\ignore}[1]{}

\newcommand{\ra}{{\rightarrow}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{eqnarray}}
\newcommand{\ea}{\end{eqnarray}}

\newcommand{\nn}{\nonumber \\}
\newcommand{\kets}[1]{ |{#1} \rangle}
\newcommand{\herr}{\gamma}
\newcommand{\segs}{{ T}}
\newcommand{\mE}{{\mathbb E}}
\newcommand{\select}[1]{\textrm{select}(#1)}

\newcommand{\rad}{\xi}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\wcomp}{\widetilde{\cal O}(N)}

\newcommand{\blu}{\color{blue}}
\newcommand{\blk}{\color{black}}
\newcommand{\red}{\color{red}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}


% In case you want to swap \lambda and \Lambda
%\let\foo\lambda
%\let\lambda\Lambda
%\let\Lambda\foo

%\input{Qcircuit}
\begin{document}
\title{Control Capacities for Quantum Systems}
\begin{abstract}
Abstract
\end{abstract}
\maketitle
\section{Introduction}
\section{Control capacity}
Control capacity provides a new way of looking at the stability of systems that builds a link between control and Shannon theory.  These links are especially evocative in quantum control because of the natural connections that occur between quantum feedback and quantum information theory.  Our goal here is to discuss how quantum control capacities can give a framework for providing a well-motivated capacity to a quantum feedback channel.

For classical systems, the actuation channel provides perhaps the most concise model for probing control capacities.  In an actualtion channel one has a system that has a state, $X$, which drifts over time either deterministically or randomly with the time $t$.  In order to perform feedback, information has to be extracted from a system.  This is achieved by function $Y(t)$, which gives an estimate of the current state of the system.  Feedback then is performed by performing an intervention on the system $U(t)$, which is determined implicitly by the output of $Y(t)$.  As a concrete example, consider the following actuation channel

\begin{align}
X(t+1) &= aX(t) + b(t)U(t),\nonumber\\
Y(t) &= X(t)
\end{align}
Here $a>0$ describes the drift of the system, which leads to exponential divergence of the state were it not for feedback.  The quantity $b(t)$ is noise in the conrol.  The measurement, $Y(t)$, is assumed to be error-free in this example but more generally errors could be considered in the model.

This model can be envisioned as a channel that maps the state $X(t)$ forward in time.  This allows us to define the capacity of a channel in terms of its ability to stabilize the moments of the of distribution of $X(t)$.  In particular, let us define $\eta^{\rm th}$ moment control capacity as

\begin{equation}
C_{\eta} :=\liminf_{t\rightarrow \infty} -\min_{U^{n-1}(\cdot)} \frac{1}{\eta t} \log\left(\left[\frac{X(t)}{X(0)}\right]^\eta \right),
\end{equation}
where the minimization over $U^{n-1}(\cdot)$ means that we optimize over all feedback strategies that are allowed within the framework.
Here $C_{\eta}$ can be interpreted as the number of most significant bits of the uncertainty in the $\eta^{\rm th}$ moment of the variable $X$ that can be dissipated by the control and feedback.  In this sense, it has the interpretation of a capacity.

There are several important values of $\eta$ for which $C_{\eta}$ is particularly significant.  The $\eta=2$ case corresponds to minimizing the second moment of the variable $X(t)$.  It is often the easiest to calculate and for this reason we focus on it in the subsequent examples.  $\lim_{\eta \rightarrow 0} C_{\eta}$ gives the Shannon control capacity, which is the weakest notion of control capacity.  The other extreme, $\lim_{\eta \rightarrow \infty} C_{\eta}$ is the zero--error control capacity.

\subsection{Quantum Control Capacities}
At a fundamental level the notion of a control capacity for quantum systems is no different than that of a control capacity for a classical system.  However, in practice the features of quantum mechanics can lead to some significant departures between the two in practice.  The first difference is what the notion of the state $X(t)$ represents.  In a classical system it usually represents a set of variables that describe the internal state of a system.  In quantum settings this is not usually possible because of the inability to cast quantum mechanics as a local hidden variable theory.  Thus we assume in this setting that if the quantum state $\ket{\psi(t)}$ represents the quantum state of a physical system, then the state $X(t)$ refers to some feature of either the quantum system or the dynamical map that the system evolves according to.

The next issue is that, unlike the actuation channel considered above, quantum mechanics comes with it an information disturbance principle which prevents us from measuring the system without damaging the wave function.  For this reason, $Y(t)$ and $X(t)$ will often have non-trivial couplings.  This makes the identification of control capacities for quantum systems more subtle than their classical counterparts.  We examine such cases below for Gaussian quantum mechanics as well as quantum rotors in the large angular momentum limit.

\section{Quantum Control Capacities for Gaussian Quantum Mechanics}
Our first example will consider calculating a second moment control capacity for a problem in Gaussian quantum mechanics.
Such processes are vitally important in optics since Gaussian states represent the minimum uncertainty pure states
of the electromagnetic field or equivalently harmonic oscillators.  Since any quantum process that maintains the Gaussian
nature of the states can be classically simulated this means that we can analytically study such cases and properly deal
with the complexities of measurement back action.

We assume that the initial state for our control problem is, for some fixed $\sigma>0$
\begin{equation}
\ket{\psi} = \int_{-\infty}^\infty \frac{e^{-x^2/4\sigma^2}}{(2\pi \sigma^2)^{1/4}}\ket{x}\mathrm{d}x\label{eq:psi0}
\end{equation}
We further assume that the free evolultion of the system is given by the following kicked Hamiltonian,
\begin{equation}
H_{\rm free}(t) = \frac{\hat{p}^2}{2m} - \frac{1}{2}\sum_{n=-\infty}^\infty \delta(2t-2n) k \hat{x}^2,
\end{equation}
which can be viewed as a linearization of the quantum kicked pendulum.  Here $\hat{p} = i \partial_x$ is the momentum operator, $m$ is the mass of the particle, $\hat{x}$ is the position operator and $k$ is the strength of the force pushing the system away from $x=0$. We choose this Hamiltonian because it leads to exponential instabilities
in the system as time increases and also because the kicked nature of the evolution makes it analytically tractable.
We assume that the correction is given by
\begin{equation}
H_{c}(t) = \sum_{n=-\infty}^\infty b_n \gamma_n p\delta(2t-[2n+1]).
\end{equation}
Here the $b_n$ are independent random variables with mean $\mu_b$ and variance $\sigma_b^2$.  The variables $\gamma_n$ will be chosen adaptively to stabilize the evolution of the state, by allowing $H_c(t)$ to translate the Gaussian back towards its rest configuration of $\bar{x} = \bra{\psi} x \ket{\psi}=0$.

Feedback requires measurement, which we model via a non-destructive Gaussian measurement.  We specifically assume that the standard deviation of the Gaussian that's projected onto by the system has standard deviation $\sigma$ but make no assumptions about the input standard deviation, $\sigma_1$, since it will deviate from $\sigma$ due to the above dynamical map.
The probability then of observing a mean value of $v_2$ given an initial Gaussian state with mean $\nu_1$ and variance $\sigma_1^2$ is
\begin{equation}
P(v_2,\sigma|v_1,\sigma_1) = \frac {{\rm e}^{-{\frac { \left( v_1-v_2 \right) ^{2}}{2\,{{
\sigma_1}}^{2}+2\,{{\sigma}}^{2}}}}}{\sqrt {{{\sigma_1}}^{2}+{{\sigma}}^{2}}
\sqrt {2\pi}}
,
\end{equation}
which is to say that we measure the state in a non-orthogonal basis that is spanned by statees of the form of~\eq{psi0}.  The value of $y$ learned from this measurement specified
the centroid of the Gaussian and allows us to discover how far the system has drifted from $\bar{x}=0$.  Also, because this form of measurement has a back action on the quantum state it captures the information disturbance feature of quantum mechanics (despite the fact that it nonetheless can be efficiently simulated by sampling).

To be clear, our objective is to stabilize $\bar{x}$ in the mean-square sense.  In order to find a linear strategy to achieve this we need to first solve the quantum dynamics of the system.
Because the map is periodic every $1$ unit of time, we only have to consider the dynamics within a single kick and feedback
\begin{align}
\ket{\psi} &\rightarrow e^{-i \hat{p}^2/2m}e^{-i\hat{p}b_n \gamma_n} e^{i k\hat{x}^2}\ket{\psi},
\end{align}
followed by a measurement operation.  Since $p=- i\partial_x$, the action of these operators on a Gaussian state can be easily computed by Fourier transformation, specifically $p=\mathcal{F} \hat{k} \mathcal{F}^{-1}$ where $\mathcal{F}$ is the unitary Fourier transformation operator and $k$ is a diagonal operator in this transformed basis that is entirely equivalent to $\hat{x}$ here (i.e. $\hat{k} \ket{k} = k\ket{k}$).  Using this trick we can write
\begin{equation}
e^{-i \hat{p}^2/2m}e^{-i\hat{p}b_n \gamma_n} e^{i k\hat{x}^2}=\mathcal{F}^{-1}e^{-i \hat{k}^2/2m}e^{-i\hat{k}b_n \gamma_n}\mathcal{F} e^{i k\hat{x}^2}.
\end{equation}
Thus the evolution for a single kicking period can be simulated by applying the phase rotation described by the kicking term, applying a Fourier transform, perform the control operation and the diffusion of the quantum system as phase rotations and then transform back to position basis by applying a Fourier transform.

While there are many initial states that can be considered here, we focus on Gaussian initial states of the form
\begin{equation}
\ket{\psi(\nu,\sigma)}:=\int_{-\infty}^\infty \frac{e^{-(x-\nu)^2/4\sigma^2}}{(2\pi \sigma^2)^{1/4}}\ket{x}\mathrm{d}x.
\end{equation}
This can easily be seen to describe a Gaussian wave function with amplitude at position $x$ of $\braket{x}{\psi(\nu,\sigma)} = \frac{e^{-(x-\nu)^2/4\sigma^2}}{(2\pi \sigma^2)^{1/4}}$.  Thus $\nu$ describes the mean of the Gaussian probability distribution that results from squaring this amplitude and $\sigma^2$ its variance.  This is convenient because the Fourier transform maps Gaussian states to Gaussians and the remaining phase rotations $e^{ik\hat{x}^2/2}$, $e^{i\hat{k}^2/2m}$ and $e^{-i hat{k} b_n \gamma_n}$ merely perturb the mean and the covariance of the resultant Gaussians.

The only operation that could potentially take us outside of this Gaussian family of wavefunctions is measurement.  However, by performing our measurement in an overcomplete basis of Gaussian functions that the probability density of a Gaussian with mean $\nu_1$ being measured as a Gaussian with mean $\nu_2$ is 
\begin{equation}
P(\nu_2,\sigma|\nu_1,\sigma_1) \propto \int_{-\infty}^\infty |\braket{\psi(\nu_2,\sigma)}{x}\braket{x}{\psi(\nu_1,\sigma_1)}|^2 \mathrm{d}x\propto  e^{-\frac{2\sigma^2 m^2 \left(\nu_2 -\left (\nu_1 + \frac{(b_n\gamma_n)m + k\nu_1}{m}\right)\right)^2}{\sigma^4(4k^2+8km+8m^2)+1}}.
\end{equation}
This gives us the probability distribution over the measured value of the mean, $\nu_2$, given that the system was initially prepared in a Gaussian with mean $\nu_1$ and variance $\sigma_1^2$.  We can compute the mean square error, over all possible resolutions of the noise, by noting that for any distribution, $P(x)$, with mean $\mu$ and variance $\sigma^2$ $\int_{-\infty}^\infty P(x)x^2 \mathrm{d}x = \mu^2 + \sigma^2$.  Using this fact in concert with the assumption that $\mathbb{E}_b (b_n) = \mu_b$ and $\mathbb{E}_b (b_n^2) = \mu_b^2 + \sigma_b^2$ we have that 
\begin{align}
&\mathbb{E}_b\left(\int_{-\infty}^\infty P(\nu_2,\sigma|\nu_1,\sigma_1) \nu_2^2\mathrm{d}\nu_2\right) \nonumber\\
&\qquad\qquad= \frac{1}{2}\frac{(1+(4k^2+8km+8m^2)\sigma^4+2\nu_1^2((1+(\mu_b^2+\sigma_b^2)\kappa^2+2\mu_b\kappa)m^2+(2\kappa\mu_b+2)km+k^2)\sigma^2)}{m^2\sigma^2}\label{eq:mse}
\end{align}
Calculus gives us that the optimal value of $\kappa$ for reducing the mean square error is
\begin{equation}
\kappa = \frac{-\mu_b(k+m)}{m(\mu_b^2 +\sigma_b^2)}.
\end{equation}
The initial mean-square error is $\nu_1^2 +\sigma^2$ before the kick operation is applied.  Thus by substituting the optimal value of $\kappa$ we see that the expected square error takes the form
\begin{equation}
\mathbb{E}_{b}\left(\int_{-\infty}^\infty P(\nu_2,\sigma|\nu_1,\sigma_1) \nu_2^2\mathrm{d}\nu_2\right) = C + \left(\frac{\sigma_b^2(k+m)^2}{m^2(\mu_b^2+\sigma_b^2)}\right)\mathbb{E}(v_1^2),
\end{equation}
where $C$ is a constant function of $\nu_1$.
Thus it follows that if we take the expectation value over all possible paths that lead from the initial state to the state $\nu_1$ we have that
\begin{equation}
\mathbb{E}_{v_1} \left(\mathbb{E}_{b}\left(\int_{-\infty}^\infty P(\nu_2,\sigma|\nu_1,\sigma_1) \nu_2^2\mathrm{d}\nu_2\right)\right) = C + \left(\frac{\sigma_b^2(k+m)^2}{m^2(\mu_b^2+\sigma_b^2)}\right)\mathbb{E}(v_1^2).
\end{equation}
Thus we are stable under repeated applications of this map if $\sigma_b < \frac{m\mu_b}{\sqrt{k^2 +2km}}$.  This value makes sense because as $k/m\rightarrow \infty$ we need to take $\sigma_b \rightarrow 0$ in order to control the system.  Furthermore, we also find that if $\mu_b=0$ we cannot control the system either because we cannot be sure that we will get the direction of the correction shifts of the Gaussian correct.

Thus the second moment quantum control capacity, when restricted to linear strategies, reduces for this problem to
\begin{equation}
C_2 = \frac{1}{2}\log \left(\left[\frac{m^2}{(k+m)^2}\right]\left[1+\frac{\mu_b^2}{\sigma_b^2} \right]\right).
\end{equation}
Note the similar form to the channel capacity of an AWGN channel, which is $\frac{1}{2} \log(1+P/N)$, where $N$ is the noise level and $P$ is the transmission power.

\end{document}